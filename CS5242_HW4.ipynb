{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMX8yuAyl55"
      },
      "source": [
        "# Welcome to CS 5242 **Homework 4**\n",
        "\n",
        "ASSIGNMENT DEADLINE ⏰ : **19 Sept 2022** \n",
        "\n",
        "In this assignment, we have three parts:\n",
        "\n",
        "1. Implement some functions in CNNs from scratch *(3 Points)*\n",
        "2. Implement a CNN and train for CIFAR10 using PyTorch *(5 Points)*\n",
        "3. Discussion (parametes and flops for AlexNet) *(2 Points)*\n",
        "\n",
        "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
        "\n",
        "> In this assignment, We **need GPU** to training the CNN model. You may need to **choose GPU in Runtime -> Change runtime type -> Hardware accerator**\n",
        "\n",
        "### **Grades Policy**\n",
        "\n",
        "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
        "\n",
        "### **Cautions**\n",
        "\n",
        "**DO NOT** use external libraries like PyTorch or TensorFlow in your implementation.\n",
        "\n",
        "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
        "\n",
        "---\n",
        "\n",
        "### **Contact**\n",
        "\n",
        "Please feel free to contact us if you have any question about this homework or need any further information.\n",
        "\n",
        "Slack (Recommend): Shenggan Cheng\n",
        "\n",
        "TA Email: shenggan@comp.nus.edu.sg\n",
        "\n",
        "> If you have not join the slack group, you can click [here](https://join.slack.com/t/cs5242ay20222-oiw1784/shared_invite/zt-1eiv24k1t-0J9EI7vz3uQmAHa68qU0aw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeZHcOVBp4U"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Start by running the cell below to set up all required software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIgu_q2HBg-E",
        "outputId": "5dee0560-b3ff-4e7a-8e81-85f3837d74ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy matplotlib torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtXcchT5H2PH"
      },
      "source": [
        "Import the neccesary library and fix seed for Python, NumPy and PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Yodsn4H6CB",
        "outputId": "8d6b3c95-ff92-4fd7-90dd-6b22b0e64351"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5cd76274d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTpFBLKSkKI0"
      },
      "source": [
        "Now let's setup the GPU environment. The colab provides a free GPU to use. Do as follows:\n",
        "\n",
        "- Runtime -> Change Runtime Type -> select `GPU` in Hardware accelerator\n",
        "- Click `connect` on the top-right\n",
        "\n",
        "After connecting to one GPU, you can check its status using `nvidia-smi` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES8KOxziiYky",
        "outputId": "631170d3-b83f-42c8-e84d-6ade6b4bb6dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 26 06:33:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yrZD7DDExF4"
      },
      "source": [
        "Everything is ready, you can move on and ***Good Luck !*** 😃"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm1f362vdRgF"
      },
      "source": [
        "## Implement functions in CNNs from scratch\n",
        "\n",
        "In this section, you need to implement some functions commonly used in CNNs, including convolution, pooling, etc. \n",
        "\n",
        "We will compare the computational results of your implemented version with those of pytorch, expecting that the error between the correct implementation and pytorch will be very small.\n",
        "\n",
        "NOTE: \n",
        "\n",
        "1. Implement these functions from scratch, **without** using any neural network libraries. Use linear algebra libraries in python is ok.\n",
        "\n",
        "2. The performance of the function is not included in this scoring, You just need to pay attention to the correctness of your implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3DXc2jgeg7"
      },
      "source": [
        "### Step 1\n",
        "Given a 32x32 pixels, 3 channels input, get a torch tensor with torch.randn()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3UxGJxTegq9O"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "x = torch.randn(batch_size, 3, 32, 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2\n",
        "\n",
        "For each following functions in the list, get the output tensor \"torch_xxx_out\" with input as x:"
      ],
      "metadata": {
        "id": "xxnlbBnFw9wB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OLQGhRbJgpIZ"
      },
      "outputs": [],
      "source": [
        "torch_max_pool = nn.MaxPool2d(kernel_size=2,\n",
        "                              stride=1,\n",
        "                              padding=0,\n",
        "                              dilation=1,\n",
        "                              return_indices=False,\n",
        "                              ceil_mode=False)\n",
        "torch_avg_pool = nn.AvgPool2d(kernel_size=2,\n",
        "                              stride=1,\n",
        "                              padding=0,\n",
        "                              ceil_mode=False,\n",
        "                              count_include_pad=True,\n",
        "                              divisor_override=None)\n",
        "torch_conv = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=6,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=0,\n",
        "                       dilation=1,\n",
        "                       groups=1,\n",
        "                       bias=True,\n",
        "                       padding_mode='zeros')\n",
        "torch_norm = nn.BatchNorm2d(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ri163mxZgvEm"
      },
      "outputs": [],
      "source": [
        "torch_sigmoid_out = torch.sigmoid(x, out=None)\n",
        "tmp_tensor = torch.randint(3, (batch_size,))\n",
        "torch_cross_entropy_out = F.cross_entropy(x[::, ::, 0, 0], tmp_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PBzDAo2rgwmx"
      },
      "outputs": [],
      "source": [
        "torch_max_pool_out = torch_max_pool(x)\n",
        "torch_avg_pool_out = torch_avg_pool(x)\n",
        "torch_conv_out = torch_conv(x)\n",
        "torch_norm_out = torch_norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6YBRP6Qgylg"
      },
      "source": [
        "### Step 3\n",
        "\n",
        "Implement these functions from scratch, without using any neural network libraries. Use linear algebra libraries in python is ok. Output your tensors as \"my_xxx_out\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CsO5I40fgzWY"
      },
      "outputs": [],
      "source": [
        "def my_max_pool(x, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
        "        kernel_size: size of the window to take a max over, \n",
        "        stride: stride of the window,\n",
        "        padding: implicit zero padding to be added on both sides,\n",
        "        \n",
        "    Return:\n",
        "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
        "    \"\"\"\n",
        "\n",
        "    y = None\n",
        "    # === Complete the code (0.5')\n",
        "    N, C_in, H_in, W_in = x.shape\n",
        "    H_out = (H_in + 2 * padding - kernel_size) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kernel_size) // stride + 1\n",
        "    np_y = np.empty((N, C_in, H_out, W_out))\n",
        "\n",
        "    padded_x = np.zeros((N, C_in, H_in + 2 * padding, W_in + 2 * padding))\n",
        "    padded_x[:, :, padding:padding + H_in, padding:padding + W_in] = x\n",
        "    for i in range(H_out):\n",
        "        for j in range(W_out):\n",
        "            np_y[:, :, i, j] = np.max(padded_x[:, :, i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size], axis=(2, 3))\n",
        "    y = torch.from_numpy(np_y)\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nGbn6oQcg4pM"
      },
      "outputs": [],
      "source": [
        "def my_avg_pool(x, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
        "        kernel_size: size of the window, \n",
        "        stride: stride of the window,\n",
        "        padding: implicit zero padding to be added on both sides,\n",
        "        \n",
        "    Return:\n",
        "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
        "    \"\"\"\n",
        "\n",
        "    y = None\n",
        "    # === Complete the code (0.5')\n",
        "    N, C_in, H_in, W_in = x.shape\n",
        "    H_out = (H_in + 2 * padding - kernel_size) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kernel_size) // stride + 1\n",
        "    np_y = np.empty((N, C_in, H_out, W_out))\n",
        "\n",
        "    padded_x = np.zeros((N, C_in, H_in + 2 * padding, W_in + 2 * padding))\n",
        "    padded_x[:, :, padding:padding + H_in, padding:padding + W_in] = x\n",
        "    for i in range(H_out):\n",
        "        for j in range(W_out):\n",
        "            np_y[:, :, i, j] = np.mean(padded_x[:, :, i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size], axis=(2, 3))\n",
        "    y = torch.from_numpy(np_y)\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9gsDytvKg5c1"
      },
      "outputs": [],
      "source": [
        "def my_conv(x, in_channels, out_channels, kernel_size, stride, padding, weight, bias):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
        "        in_channels: number of channels in the input image, it is C_in;\n",
        "        out_channels: number of channels produced by the convolution;\n",
        "        kernel_size: size of onvolving kernel, \n",
        "        stride: stride of the convolution,\n",
        "        padding: implicit zero padding to be added on both sides of each dimension,\n",
        "        \n",
        "    Return:\n",
        "        y: torch tensor of size (N, C_out, H_out, W_out)\n",
        "    \"\"\"\n",
        "\n",
        "    y = None\n",
        "    # === Complete the code (0.5')\n",
        "    N, C_in, H_in, W_in = x.shape\n",
        "    H_out = (H_in + 2 * padding - kernel_size) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kernel_size) // stride + 1\n",
        "    np_y = np.empty((N, out_channels, H_out, W_out))\n",
        "\n",
        "    padded_x = np.zeros((N, in_channels, H_in + 2 * padding, W_in + 2 * padding))\n",
        "    padded_x[:, :, padding:padding + H_in, padding:padding + W_in] = x\n",
        "    for i in range(H_out):\n",
        "        for j in range(W_out):\n",
        "          array_to_pool = padded_x[:, :, i*stride : i*stride + kernel_size, j*stride : j*stride + kernel_size]\n",
        "          reshape_array = array_to_pool.reshape(N, -1)\n",
        "          reshape_weight = weight.reshape(out_channels, -1)\n",
        "          np_y[:, :, i, j] = np.matmul(reshape_array, reshape_weight.T.detach().numpy()) + bias.detach().numpy()\n",
        "    y = torch.from_numpy(np_y)\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8sX0oRyTg-m6"
      },
      "outputs": [],
      "source": [
        "def my_batchnorm(x, num_features, eps):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C, H, W),\n",
        "        num_features: number of features in the input tensor, it is C;\n",
        "        eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
        "        \n",
        "    Return:\n",
        "        y: torch tensor of size (N, C, H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    y = torch.empty_like(x)\n",
        "    # === Complete the code (0.5')\n",
        "    gamma = np.ones((1, num_features, 1, 1))\n",
        "    beta = np.zeros((1, num_features, 1, 1))\n",
        "    mean = np.mean(x.detach().numpy(), axis=(0, 2, 3), keepdims=True)\n",
        "    var = np.var(x.detach().numpy(), axis=(0, 2, 3), keepdims=True)\n",
        "    np_y = gamma * (x.detach().numpy() - mean) / np.sqrt(var + eps) + beta\n",
        "    y = torch.from_numpy(np_y)\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZqwnIjPOhCQm"
      },
      "outputs": [],
      "source": [
        "def my_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with any size\n",
        "\n",
        "    Return:\n",
        "        y: the logistic sigmoid function of x\n",
        "    \"\"\"\n",
        "    y = None\n",
        "    # === Complete the code (0.5')\n",
        "    y = torch.from_numpy(1 / (1 + np.exp(-x.detach().numpy())))\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZPIFPl3ehEFh"
      },
      "outputs": [],
      "source": [
        "def my_cross_entropy(p, y):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        p: torch tensor with size of (N, C),\n",
        "        y (int): torch tensor with size of (N), the values range from 0 to C-1\n",
        "\n",
        "    Return:\n",
        "        loss: the cross_entropy of predicted values p and target y.\n",
        "    \"\"\"\n",
        "    loss = None\n",
        "    # === Complete the code (0.5')\n",
        "    softmax_probs = np.exp(p.detach().numpy()) / np.sum(np.exp(p.detach().numpy()), axis=1, keepdims=True)\n",
        "    loss = torch.from_numpy(np.asarray(np.mean(-np.log(softmax_probs[range(p.shape[0]), y]))))\n",
        "    # === Complete the code\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dMnKzeVuhGxu"
      },
      "outputs": [],
      "source": [
        "my_max_pool_out = my_max_pool(x, kernel_size=2, stride=1, padding=0)\n",
        "my_avg_pool_out = my_avg_pool(x, kernel_size=2, stride=1, padding=0)\n",
        "my_conv_out = my_conv(x,\n",
        "                      in_channels=3,\n",
        "                      out_channels=6,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=0,\n",
        "                      weight=torch_conv.weight,\n",
        "                      bias=torch_conv.bias)\n",
        "my_norm_out = my_batchnorm(x, num_features=3, eps=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yUOZJ875hLAD"
      },
      "outputs": [],
      "source": [
        "my_sigmoid_out = my_sigmoid(x)\n",
        "my_cross_entropy_out = my_cross_entropy(x[::, ::, 0, 0], tmp_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO-EHT7wm7Dk"
      },
      "source": [
        "### Step 4\n",
        "\n",
        "Compare and show that \"torch_xxx_out\" and \"my_xxx_out\" are equal up to small numerical errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXnNfKKJhOAi",
        "outputId": "d6661000-a2c1-448d-a656-14110fc81c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0., dtype=torch.float64)\n",
            "tensor(4.2491e-16, dtype=torch.float64)\n",
            "tensor(3.1710e-15, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
            "tensor(2.2439e-15, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(F.mse_loss(my_max_pool_out, torch_max_pool_out))\n",
        "print(F.mse_loss(my_avg_pool_out, torch_avg_pool_out))\n",
        "print(F.mse_loss(my_conv_out, torch_conv_out))\n",
        "print(F.mse_loss(my_norm_out, torch_norm_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHxuCzkmhP4l",
        "outputId": "407a056f-3cba-40af-fc5b-6df760b55e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.6253e-16)\n",
            "tensor(0.)\n"
          ]
        }
      ],
      "source": [
        "print(F.mse_loss(my_sigmoid_out, torch_sigmoid_out))\n",
        "print(F.mse_loss(my_cross_entropy_out, torch_cross_entropy_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFZB_A7ddrC"
      },
      "source": [
        "## Train CNNs on CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a CNN and train for CIFAR10 with these definitions:\n",
        "\n",
        "1. cA-B = Conv2d with input A channels, output B channels - kernel size 3x3, stride (1,1), padding with zeros to keep image size constant, followed by ReLU;\n",
        "\n",
        "2. mp = maxpool2d kernel size 2x2, stride (2,2);\n",
        "\n",
        "3. bn = batchnorm2d with affine=False (i.e. non learning batch norm);\n",
        "\n",
        "4. fcA-B = nn.linear with input A nodes, output B nodes;\n",
        "\n",
        "5. aap = adaptive average pooling.\n",
        "\n",
        "Use the definition to make the architecture c3-16 -> c16-16 -> mp -> c16-32 -> c32-32 -> mp -> c32-64 -> c64-64 -> mp -> c64-128 -> c128-128 -> aap -> flatten -> fc128-10 -> cross entropy loss. Adjust learning rate, batch size and other hyper parameters to make classification results **> 75%**."
      ],
      "metadata": {
        "id": "88y6cXmPs7_E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HeUlgbGgg9JI"
      },
      "outputs": [],
      "source": [
        "# === Complete the code (1')\n",
        "num_epoch = 20 # TODO: please define the number of epoch here.\n",
        "batch_size = 64 # TODO: please fill the batch size here.\n",
        "# === Complete the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "0344adc4603c42debb0895eb24d52daa",
            "857ca08fc8b94375be2a47068d8af875",
            "cc866ebc1f3f40e68d843c8839cbcd91",
            "c4931d9ede0f4ad7aef1975a44fa8253",
            "1fa381ca7db4401ca9a2967910997fd1",
            "b3e536af091b40bda4c152a00229e374",
            "2c876cc644304defa9686bb36e0b6ebd",
            "0f87cb66d0ab440a9c51f1fc3e76f32d",
            "472396db87d04c2d896074e296278b9e",
            "29b973eaf531459db9780efe49f5b1fd",
            "4663a3a21c734f5591c1782a42ac067a"
          ]
        },
        "id": "e9DLvGs6hmLJ",
        "outputId": "4e9a9431-482d-46fd-e109-bc9feb609314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0344adc4603c42debb0895eb24d52daa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                       train=False,\n",
        "                                       download=True,\n",
        "                                       transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o0VD5zmxhvkT"
      },
      "outputs": [],
      "source": [
        "# Creating a CNN model\n",
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "       \n",
        "        # === Complete the code (1.5')\n",
        "        self.c3_16 = nn.Conv2d(3, 16, (3,3), 1, 1)\n",
        "        self.c16_16 = nn.Conv2d(16, 16, (3,3), 1, 1)\n",
        "        self.c16_32 = nn.Conv2d(16, 32, (3,3), 1, 1)\n",
        "        self.c32_32 = nn.Conv2d(32, 32, (3,3), 1, 1)\n",
        "        self.c32_64 = nn.Conv2d(32, 64, (3,3), 1, 1)\n",
        "        self.c64_64 = nn.Conv2d(64, 64, (3,3), 1, 1)\n",
        "        self.c64_128 = nn.Conv2d(64, 128, (3,3), 1, 1)\n",
        "        self.c128_128 = nn.Conv2d(128, 128, (3,3), 1, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.mp = nn.MaxPool2d(2, 2)\n",
        "        self.aap = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc128_10 = nn.Linear(128, num_classes)\n",
        "        \n",
        "        # === Complete the code\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # === Complete the code (1.5')\n",
        "        out = self.relu(self.c3_16(x))\n",
        "        out = self.relu(self.c16_16(out))\n",
        "        out = self.mp(out)\n",
        "        out = self.relu(self.c16_32(out))\n",
        "        out = self.relu(self.c32_32(out))\n",
        "        out = self.mp(out)\n",
        "        out = self.relu(self.c32_64(out))\n",
        "        out = self.relu(self.c64_64(out))\n",
        "        out = self.mp(out)\n",
        "        out = self.relu(self.c64_128(out))\n",
        "        out = self.relu(self.c128_128(out))\n",
        "        out = self.aap(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc128_10(out)\n",
        "        # === Complete the code\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Q8LfQbxThx_0"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN(10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYClpDydh0qA",
        "outputId": "a79f6b2c-b3be-47be-e8e9-c7d41260ca04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch   0 |   128 batches loss: 2.1503\n",
            "epoch   0 |   256 batches loss: 1.9540\n",
            "epoch   0 |   384 batches loss: 1.8024\n",
            "epoch   0 |   512 batches loss: 1.7085\n",
            "epoch   0 |   640 batches loss: 1.6382\n",
            "epoch   0 |   768 batches loss: 1.5766\n",
            "epoch   1 |   128 batches loss: 1.5202\n",
            "epoch   1 |   256 batches loss: 1.4625\n",
            "epoch   1 |   384 batches loss: 1.4347\n",
            "epoch   1 |   512 batches loss: 1.3864\n",
            "epoch   1 |   640 batches loss: 1.3194\n",
            "epoch   1 |   768 batches loss: 1.2890\n",
            "epoch   2 |   128 batches loss: 1.2686\n",
            "epoch   2 |   256 batches loss: 1.2431\n",
            "epoch   2 |   384 batches loss: 1.2300\n",
            "epoch   2 |   512 batches loss: 1.1967\n",
            "epoch   2 |   640 batches loss: 1.1782\n",
            "epoch   2 |   768 batches loss: 1.1417\n",
            "epoch   3 |   128 batches loss: 1.1243\n",
            "epoch   3 |   256 batches loss: 1.1070\n",
            "epoch   3 |   384 batches loss: 1.0907\n",
            "epoch   3 |   512 batches loss: 1.0766\n",
            "epoch   3 |   640 batches loss: 1.0439\n",
            "epoch   3 |   768 batches loss: 1.0600\n",
            "epoch   4 |   128 batches loss: 1.0168\n",
            "epoch   4 |   256 batches loss: 0.9976\n",
            "epoch   4 |   384 batches loss: 0.9910\n",
            "epoch   4 |   512 batches loss: 0.9926\n",
            "epoch   4 |   640 batches loss: 0.9720\n",
            "epoch   4 |   768 batches loss: 0.9585\n",
            "epoch   5 |   128 batches loss: 0.9316\n",
            "epoch   5 |   256 batches loss: 0.9187\n",
            "epoch   5 |   384 batches loss: 0.8966\n",
            "epoch   5 |   512 batches loss: 0.8873\n",
            "epoch   5 |   640 batches loss: 0.9013\n",
            "epoch   5 |   768 batches loss: 0.8914\n",
            "epoch   6 |   128 batches loss: 0.8445\n",
            "epoch   6 |   256 batches loss: 0.8384\n",
            "epoch   6 |   384 batches loss: 0.8393\n",
            "epoch   6 |   512 batches loss: 0.8279\n",
            "epoch   6 |   640 batches loss: 0.8355\n",
            "epoch   6 |   768 batches loss: 0.8406\n",
            "epoch   7 |   128 batches loss: 0.7792\n",
            "epoch   7 |   256 batches loss: 0.8084\n",
            "epoch   7 |   384 batches loss: 0.8099\n",
            "epoch   7 |   512 batches loss: 0.7887\n",
            "epoch   7 |   640 batches loss: 0.7510\n",
            "epoch   7 |   768 batches loss: 0.7861\n",
            "epoch   8 |   128 batches loss: 0.7378\n",
            "epoch   8 |   256 batches loss: 0.7432\n",
            "epoch   8 |   384 batches loss: 0.7474\n",
            "epoch   8 |   512 batches loss: 0.7137\n",
            "epoch   8 |   640 batches loss: 0.7286\n",
            "epoch   8 |   768 batches loss: 0.7090\n",
            "epoch   9 |   128 batches loss: 0.6868\n",
            "epoch   9 |   256 batches loss: 0.6880\n",
            "epoch   9 |   384 batches loss: 0.6814\n",
            "epoch   9 |   512 batches loss: 0.6899\n",
            "epoch   9 |   640 batches loss: 0.6753\n",
            "epoch   9 |   768 batches loss: 0.6693\n",
            "epoch  10 |   128 batches loss: 0.6401\n",
            "epoch  10 |   256 batches loss: 0.6513\n",
            "epoch  10 |   384 batches loss: 0.6364\n",
            "epoch  10 |   512 batches loss: 0.6524\n",
            "epoch  10 |   640 batches loss: 0.6379\n",
            "epoch  10 |   768 batches loss: 0.6489\n",
            "epoch  11 |   128 batches loss: 0.5868\n",
            "epoch  11 |   256 batches loss: 0.6083\n",
            "epoch  11 |   384 batches loss: 0.6058\n",
            "epoch  11 |   512 batches loss: 0.6117\n",
            "epoch  11 |   640 batches loss: 0.6098\n",
            "epoch  11 |   768 batches loss: 0.6111\n",
            "epoch  12 |   128 batches loss: 0.5647\n",
            "epoch  12 |   256 batches loss: 0.5567\n",
            "epoch  12 |   384 batches loss: 0.5471\n",
            "epoch  12 |   512 batches loss: 0.5719\n",
            "epoch  12 |   640 batches loss: 0.5821\n",
            "epoch  12 |   768 batches loss: 0.5604\n",
            "epoch  13 |   128 batches loss: 0.5307\n",
            "epoch  13 |   256 batches loss: 0.5265\n",
            "epoch  13 |   384 batches loss: 0.5248\n",
            "epoch  13 |   512 batches loss: 0.5394\n",
            "epoch  13 |   640 batches loss: 0.5216\n",
            "epoch  13 |   768 batches loss: 0.5438\n",
            "epoch  14 |   128 batches loss: 0.4877\n",
            "epoch  14 |   256 batches loss: 0.4782\n",
            "epoch  14 |   384 batches loss: 0.4952\n",
            "epoch  14 |   512 batches loss: 0.5073\n",
            "epoch  14 |   640 batches loss: 0.4987\n",
            "epoch  14 |   768 batches loss: 0.5030\n",
            "epoch  15 |   128 batches loss: 0.4675\n",
            "epoch  15 |   256 batches loss: 0.4412\n",
            "epoch  15 |   384 batches loss: 0.4596\n",
            "epoch  15 |   512 batches loss: 0.4559\n",
            "epoch  15 |   640 batches loss: 0.4781\n",
            "epoch  15 |   768 batches loss: 0.4732\n",
            "epoch  16 |   128 batches loss: 0.4232\n",
            "epoch  16 |   256 batches loss: 0.4215\n",
            "epoch  16 |   384 batches loss: 0.4201\n",
            "epoch  16 |   512 batches loss: 0.4371\n",
            "epoch  16 |   640 batches loss: 0.4409\n",
            "epoch  16 |   768 batches loss: 0.4492\n",
            "epoch  17 |   128 batches loss: 0.3930\n",
            "epoch  17 |   256 batches loss: 0.3920\n",
            "epoch  17 |   384 batches loss: 0.4010\n",
            "epoch  17 |   512 batches loss: 0.4109\n",
            "epoch  17 |   640 batches loss: 0.4154\n",
            "epoch  17 |   768 batches loss: 0.3955\n",
            "epoch  18 |   128 batches loss: 0.3607\n",
            "epoch  18 |   256 batches loss: 0.3630\n",
            "epoch  18 |   384 batches loss: 0.3860\n",
            "epoch  18 |   512 batches loss: 0.3822\n",
            "epoch  18 |   640 batches loss: 0.3690\n",
            "epoch  18 |   768 batches loss: 0.3836\n",
            "epoch  19 |   128 batches loss: 0.3395\n",
            "epoch  19 |   256 batches loss: 0.3330\n",
            "epoch  19 |   384 batches loss: 0.3227\n",
            "epoch  19 |   512 batches loss: 0.3623\n",
            "epoch  19 |   640 batches loss: 0.3542\n",
            "epoch  19 |   768 batches loss: 0.3537\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        # === Complete the code (1')\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model.forward(inputs)\n",
        "        loss = criterion(y_pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # === Complete the code\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 128 == 0:\n",
        "            print('epoch {:3d} | {:5d} batches loss: {:.4f}'.format(epoch, i + 1, running_loss/128))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kuzhd2zWh415"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0_3yVTTh-U6",
        "outputId": "b1145a0d-8311-460a-8e6c-5ccae594b474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 76 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        #images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z53XxMASdm-A"
      },
      "source": [
        "## Discussion (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpbGzIo9dqWN"
      },
      "source": [
        "Calculate Parameters and FLOPs(Floating point operations) of **AlexNet** and analyse the ratio of the number of parameters and the amount of calculations for different layers in AlexNet.\n",
        "\n",
        "Hint:\n",
        "\n",
        "1. You can refer https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html for architecture of AlexNet.\n",
        "2. You only need to make estimates and do not need to perform rigorous calculations, (e.g. only consider the FLOPs of the convolution and FC in AlexNet model)\n",
        "3. Because Multiply Accumulate (MAC) operations are performed on the hardware, it is possible to simply consider only the number of multiplications when considering the number of operations when calculating FLOPs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AlexNet architecture is as follows: <br>\n",
        "```python\n",
        "self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "```"
      ],
      "metadata": {
        "id": "PB9Y3Ag5fPnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Number of Parameters\n",
        "  * The number of parameters is calculated by the sum of the weights and the bias in each layer.\n",
        "  * The number of weight parameters are the number of conv/fc kernels * size of kernel * number of input channels\n",
        "  * The bias has the same size as the number of kernels.\n",
        "  * Max pooling and normalisation layers do not have any parameters associated with them.\n",
        "\n",
        "* Number of Floating Point Operations (FLOPs)\n",
        "  * The number of FLOPs is calculated as the product of the dimensions of the weights by the size of the outputs (as these represent the number of times the kernel performs its multiplications).\n",
        "  * FLOPs for max pooling and normalisation layers are not calculated."
      ],
      "metadata": {
        "id": "t50Z_KOHT87V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Layer | Stride | Padding |  Input Size | Kernel Size | Output Size | Weights Calculation | Weights | Bias | # Parameters | FLOPs Calculation | # FLOPs | # Parameters / # FLOPs\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| Conv | 4 | 2 | [224x224x3] | [11x11x3] | [55x55x64] | 64 * [11x11x3] | 23232 | 64 | 23296 | [55x55]x64x[11x11x3]+64x3 | 70276992 | 3.3 * e-3\n",
        "| MaxPool | 2 | 0 | [55x55x64] | [3x3] | [27x27x64] | - | - | - | 0 | - | 0 | -\n",
        "| Conv | 1 | 2 | [27x27x64] | [5x5x64] | [27x27x192] | 192 * [5x5x64] | 307200 | 192 | 307392 | [27x27]x192x[5x5x64]+192x64 | 223961088 | 1.3 * e-3\n",
        "| MaxPool | 2 | 0 | [27x27x192] | [3x3] | [13x13x192] | - | - | - | 0 | - | 0 | -\n",
        "| Conv | 1 | 1 | [13x13x192] | [3x3x192] | [13x13x384] | 384 * [3x3x192] | 663552 | 384 | 663936 | [13x13]x384x[3x3x192]+192x384 | 112214016 | 5.9 * e-3\n",
        "| Conv | 1 | 1 | [13x13x384] | [3x3x384] | [13x13x256] | 256 * [3x3x384] | 884736 | 256 | 884992 | [13x13]x256x[3x3x384]+384x256 | 149618688 | 5.9 * e-3\n",
        "| Conv | 1 | 1 | [13x13x256] | [3x3x256] | [13x13x256] | 256 * [3x3x256] | 589824 | 256 | 590080 | [13x13]x256x[3x3x256]+256x256 | 99745792 | 5.9 * e-3\n",
        "| MaxPool | 2 | 0 | [13x13x256] | [3x3] | [6x6x256] | - | - | - | 0 | - | 0 | -\n",
        "| FC | - | - | [6x6x256] | - | [4096x1] | 4096 * [6x6x256] | 37748736 | 4096 | 37752832 | [4096x1]x[6x6x256] + 4096 | 37752832 | 1\n",
        "| FC | - | - | [4096x1] | - | [4096x1] | 4096 * [4096x1] | 16777216 | 4096 | 16781312 | [4096x1]x[4096x1] + 4096 | 16781312 | 1\n",
        "| FC | - | - | [4096x1] | - | [1000x1] | 1000 * [4096x1] | 4096000 | 1000 | 4097000 | [4096x1]x[1000x1] + 1000 | 4097000 | 1"
      ],
      "metadata": {
        "id": "DWlImSIqHmay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total number of parameters is 61100840 ~ 61 million. <br>\n",
        "The total number of FLOPs is 714447720 ~ 714 million."
      ],
      "metadata": {
        "id": "DeAMI_dSLr8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Convolutional Layers\n",
        "  * For a Convolutional Layer, the number of FLOPs is proportional to the number of parameters multiplied by the 2 dimensional size of the output.\n",
        "  * The magnitude of all ratios for convolutional layers is approximately around 4e-3\n",
        "  * We can see that a smaller kernel size leads to a larger ratio of parameters to FLOPs, because it increases the number of multiplication operations required.\n",
        "* Fully Connected Layers\n",
        "  * For Fully Connected Layers, we can see that the number of parameters is equal to the number of Floating Point Operations (FLOPs).\n",
        "  * This is expected as the presence of no external kernel makes each parameter contribute to one operation.\n",
        "* From these, it appears that the number of FLOPs is only larger when the layer creates newer transformations/complexities (features) from our inputs"
      ],
      "metadata": {
        "id": "ku62-RoEZ0r0"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0344adc4603c42debb0895eb24d52daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_857ca08fc8b94375be2a47068d8af875",
              "IPY_MODEL_cc866ebc1f3f40e68d843c8839cbcd91",
              "IPY_MODEL_c4931d9ede0f4ad7aef1975a44fa8253"
            ],
            "layout": "IPY_MODEL_1fa381ca7db4401ca9a2967910997fd1"
          }
        },
        "857ca08fc8b94375be2a47068d8af875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3e536af091b40bda4c152a00229e374",
            "placeholder": "​",
            "style": "IPY_MODEL_2c876cc644304defa9686bb36e0b6ebd",
            "value": "100%"
          }
        },
        "cc866ebc1f3f40e68d843c8839cbcd91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f87cb66d0ab440a9c51f1fc3e76f32d",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_472396db87d04c2d896074e296278b9e",
            "value": 170498071
          }
        },
        "c4931d9ede0f4ad7aef1975a44fa8253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b973eaf531459db9780efe49f5b1fd",
            "placeholder": "​",
            "style": "IPY_MODEL_4663a3a21c734f5591c1782a42ac067a",
            "value": " 170498071/170498071 [00:14&lt;00:00, 12776954.77it/s]"
          }
        },
        "1fa381ca7db4401ca9a2967910997fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e536af091b40bda4c152a00229e374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c876cc644304defa9686bb36e0b6ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f87cb66d0ab440a9c51f1fc3e76f32d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "472396db87d04c2d896074e296278b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29b973eaf531459db9780efe49f5b1fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4663a3a21c734f5591c1782a42ac067a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}