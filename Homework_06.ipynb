{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to CS 5242 **Homework 6**\n",
        "\n",
        "ASSIGNMENT DEADLINE ⏰ : **23 October 23:59** \n",
        "\n",
        "In this assignment, we have four questions. Write the answers to each question in this notebook.\n",
        "\n",
        "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
        "\n",
        "### **Grades Policy**\n",
        "\n",
        "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
        "\n",
        "### **Cautions**\n",
        "\n",
        "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
        "\n",
        "---\n",
        "\n",
        "### **Contact**\n",
        "\n",
        "Please feel free to contact us if you have any question about this homework or need any further information.\n",
        "\n",
        "Slack (Recommend): Kin Whye Chew\n",
        "\n",
        "TA Email: kinwhye@nus.edu.sg\n",
        "\n",
        "> If you have not join the slack group, you can click [here](https://join.slack.com/t/cs5242ay20222-oiw1784/shared_invite/zt-1eiv24k1t-0J9EI7vz3uQmAHa68qU0aw)"
      ],
      "metadata": {
        "id": "DDilIPeouuJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Google Colaboratory\n",
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    # mount google drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "    file_name = 'Homework_06.ipynb'\n",
        "    path_to_file = '/content/gdrive/My Drive/Homework06' # Please adjust the path accordingly \n",
        "    print(path_to_file)\n",
        "    # change current path to the folder containing \"file_name\"\n",
        "    os.chdir(path_to_file)\n",
        "    !pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OK0V5flfdf1",
        "outputId": "246351f1-198c-44ff-b0a8-f3807bf692fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Homework06\n",
            "/content/gdrive/My Drive/Homework06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract dataset"
      ],
      "metadata": {
        "id": "Hm8iGWGngefH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "JZwWG_OYf14h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa1frUWBezF3"
      },
      "source": [
        "## Question 1 (1 Point)\n",
        "\n",
        "Implement and train a vanilla recurrent neural network (VRNN) for predicting the next world in a sequence. The  dataset is a subset of PTB composed of 20 sub-documents with each training document having 1000 words and each test document having 1000 words as well. \n",
        "\n",
        "__Requirements/Grading:__\n",
        "1. Find the hyperparameters to obtain test perplexity smaller than 400.\n",
        "\n",
        "Hint: You may choose your own values for the hyper-parameters, except for the number of epochs. You can use torch.nn.RNN. You can use the test perplexity for hyper-parameter tuning. Think about which parameter would have a huge impact on the performance and tune it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X7NwNry3ezF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad110fc-c257-456d-e623-d2c366556bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestamp: 22-10-23--04-07-28\n",
            "torch.Size([1000, 20]) torch.Size([1000, 20])\n",
            "cuda\n",
            "three_layer_recurrent_net(\n",
            "  (layer1): Embedding(10000, 72)\n",
            "  (layer2): RNN(72, 72)\n",
            "  (layer3): Linear(in_features=72, out_features=10000, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "epoch= 0 \t time= 1.9191410541534424 \t lr= 1 \t exp(loss)= 1055.120844837389\n",
            "test: exp(loss) =  809.2285213969394\n",
            "\n",
            "epoch= 1 \t time= 2.677746534347534 \t lr= 1 \t exp(loss)= 656.5790188994383\n",
            "test: exp(loss) =  714.1923201687526\n",
            "\n",
            "epoch= 2 \t time= 3.423279285430908 \t lr= 1 \t exp(loss)= 494.39850198231676\n",
            "test: exp(loss) =  648.4287499327119\n",
            "\n",
            "epoch= 3 \t time= 4.181236267089844 \t lr= 1 \t exp(loss)= 384.8681475740188\n",
            "test: exp(loss) =  622.5897277322877\n",
            "\n",
            "epoch= 4 \t time= 4.945204734802246 \t lr= 0.9090909090909091 \t exp(loss)= 297.8934477445862\n",
            "test: exp(loss) =  605.9112480979009\n"
          ]
        }
      ],
      "source": [
        "%reset -f\n",
        "import torch\n",
        "import datetime\n",
        "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
        "\n",
        "train_data, test_data = torch.load('dataset/small_PTB.pt')\n",
        "print(train_data.size(), test_data.size())\n",
        "\n",
        "bs = 20\n",
        "vocab_size = 10000\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# In order to obtain a test perplexity of less than 400, some hyperparameter tuning has to be performed\n",
        "# In this scenario, we're using the test set to perform hyperparamter tuning. In practice, this is NEVER allowed\n",
        "# When we do hyperparameter tuning with the test set, the hyperparameters will overfit to the test set, and therefore the test set\n",
        "# will no longer be an unbiased estimate of the generalization error\n",
        "\n",
        "num_epochs = 5\n",
        "# Here, we only tune the hidden size, lr, and seq length.\n",
        "# Looking through the code, you can find other hyperparameters to tune, such as the optimizer, number of layers, lr_decay, weight_decay, etc\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "hidden_size = 72\n",
        "my_lr = 1\n",
        "seq_length = 3\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "# The code below is taken fromthe VRNN demo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "class three_layer_recurrent_net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(three_layer_recurrent_net, self).__init__()\n",
        "        \n",
        "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
        "        self.layer2 = nn.RNN(       hidden_size , hidden_size  )\n",
        "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
        "\n",
        "        \n",
        "    def forward(self, word_seq, h_init ):\n",
        "        \n",
        "        g_seq               =   self.layer1( word_seq )  \n",
        "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
        "        score_seq           =   self.layer3( h_seq )\n",
        "        \n",
        "        return score_seq,  h_final \n",
        "\n",
        "net = three_layer_recurrent_net( hidden_size )\n",
        "\n",
        "print(net)\n",
        "\n",
        "net = net.to(device)\n",
        "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "print('')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def normalize_gradient(net):\n",
        "\n",
        "    grad_norm_sq=0\n",
        "\n",
        "    for p in net.parameters():\n",
        "        grad_norm_sq += p.grad.data.norm()**2\n",
        "\n",
        "    grad_norm=math.sqrt(grad_norm_sq)\n",
        "\n",
        "    if grad_norm<1e-4:\n",
        "        net.zero_grad()\n",
        "        print('grad norm close to zero')\n",
        "    else:    \n",
        "        for p in net.parameters():\n",
        "            p.grad.data.div_(grad_norm)\n",
        "\n",
        "    return grad_norm\n",
        "def eval_on_test_set():\n",
        "\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "    \n",
        "    h = torch.zeros(1, bs, hidden_size)\n",
        "    \n",
        "    h=h.to(device)\n",
        "\n",
        "    \n",
        "    for count in range( 0 , 999-seq_length ,  seq_length) :\n",
        "            \n",
        "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
        "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
        "        \n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "                                \n",
        "        scores, h  = net( minibatch_data, h )\n",
        "        \n",
        "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
        "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
        "        \n",
        "        loss = criterion(  scores ,  minibatch_label )    \n",
        "        \n",
        "        h=h.detach()\n",
        "            \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1        \n",
        "    \n",
        "    total_loss = running_loss/num_batches \n",
        "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
        "    return math.exp(total_loss)\n",
        "\n",
        "start=time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
        "    if epoch >= 4:\n",
        "        my_lr = my_lr / 1.1\n",
        "    \n",
        "    # create a new optimizer and give the current learning rate.   \n",
        "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
        "        \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "    \n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(1, bs, hidden_size)\n",
        "\n",
        "    # send it to the gpu    \n",
        "    h=h.to(device)\n",
        "    \n",
        "    for count in range( 0 , 999-seq_length ,  seq_length):\n",
        "            \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
        "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
        "        \n",
        "        # send them to the gpu\n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h=h.detach()\n",
        "        # h=h.requires_grad_()\n",
        "                    \n",
        "        # forward the minibatch through the net        \n",
        "        scores, h  = net( minibatch_data, h )\n",
        "        \n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
        "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(  scores ,  minibatch_label )\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        \n",
        "            \n",
        "        # update the running loss  \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        \n",
        "        \n",
        "    # compute stats for the full training set\n",
        "    total_loss = running_loss/num_batches\n",
        "    elapsed = time.time()-start\n",
        "    \n",
        "    print('')\n",
        "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
        "    test_loss = eval_on_test_set() \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Aot53UezF4"
      },
      "source": [
        "## Question 2 (2 Point)\n",
        "\n",
        "Implement and train a vanilla recurrent neural network (VRNN) on the small PTB dataset by **explicitly** implementing the VRNN layer (the use of the function nn.RNN() is prohibited) :\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "h_t =& \\textrm{ tanh}(Ah_{t-1}+a+Bx_t+b)\\\\\n",
        "y_t =& \\ C h_{t} +c\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $(A,a)$ are the parameters of the linear transformation (matrix,bias) applied to $h_{t-1}$, $(B,b)$ are the parameters of the linear transformation applied to $x_t$ and $(C,c)$ are the parameters of the linear transformation applied to $h_t$.\n",
        "\n",
        "The  dataset is a subset of PTB composed of 20 sub-documents with each training document having 1000 words and each test document having 1000 words as well. \n",
        "\n",
        "__Requirements/Grading:__\n",
        "1. Explicitly implement a vanilla recurrent neural network\n",
        "**Hints**: \n",
        "1. Activation function tanh is given by *torch.tanh*\n",
        "1. You may consider creating a list of $h_t$ with `h_seq = []`, add a vector $h_t$ to the list with `h_seq.append(h_t)` and convert the list of vectors into a PyTorch tensor with `h_seq = torch.stack(h_seq, dim=0).squeeze()`.\n",
        "1. You can reuse the hyperparameters from question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fEVHioTUezF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece4faaa-35fb-4098-915f-2f413c24ceb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestamp: 22-10-23--04-07-46\n",
            "torch.Size([1000, 20]) torch.Size([1000, 20])\n",
            "cuda\n",
            "three_layer_recurrent_net(\n",
            "  (layer1): Embedding(10000, 12)\n",
            "  (layer2): Linear(in_features=12, out_features=12, bias=True)\n",
            "  (layer3): Linear(in_features=12, out_features=10000, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "epoch= 0 \t time= 0.6949219703674316 \t lr= 1 \t exp(loss)= 992.1406888343638\n",
            "test: exp(loss) =  824.7652334489339\n",
            "\n",
            "epoch= 1 \t time= 1.5672154426574707 \t lr= 1 \t exp(loss)= 724.3993703268663\n",
            "test: exp(loss) =  782.3126216570743\n",
            "\n",
            "epoch= 2 \t time= 2.443049192428589 \t lr= 1 \t exp(loss)= 650.7906748479563\n",
            "test: exp(loss) =  744.0545370855605\n",
            "\n",
            "epoch= 3 \t time= 3.313577651977539 \t lr= 1 \t exp(loss)= 579.9729808018067\n",
            "test: exp(loss) =  709.3443072077758\n",
            "\n",
            "epoch= 4 \t time= 4.1717236042022705 \t lr= 0.9090909090909091 \t exp(loss)= 520.497642956948\n",
            "test: exp(loss) =  684.4066584089579\n"
          ]
        }
      ],
      "source": [
        "%reset -f\n",
        "import torch\n",
        "import datetime\n",
        "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
        "\n",
        "train_data, test_data = torch.load('dataset/small_PTB.pt')\n",
        "print(train_data.size(), test_data.size())\n",
        "\n",
        "bs = 20\n",
        "vocab_size = 10000\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# The the RNN of the three_layer_recurrent_net has to be explicitly implemented\n",
        "# YOUR CODE STARTS HERE \n",
        "class three_layer_recurrent_net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(three_layer_recurrent_net, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layer1 = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, word_seq, h_init ):        \n",
        "        h_seq = []\n",
        "        h_t = h_init\n",
        "        for word in word_seq:\n",
        "            h_t = torch.tanh(self.layer1(word) + self.layer2(h_t))\n",
        "            h_seq.append(h_t)\n",
        "        h_seq = torch.stack(h_seq, dim=0).squeeze()\n",
        "        y_seq = self.layer3(h_seq)\n",
        "        return y_seq, h_t\n",
        "        \n",
        "hidden_size = 12\n",
        "my_lr = 1\n",
        "seq_length = 3\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "net = three_layer_recurrent_net( hidden_size )\n",
        "\n",
        "print(net)\n",
        "\n",
        "net = net.to(device)\n",
        "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "print('')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def normalize_gradient(net):\n",
        "\n",
        "    grad_norm_sq=0\n",
        "\n",
        "    for p in net.parameters():\n",
        "        grad_norm_sq += p.grad.data.norm()**2\n",
        "\n",
        "    grad_norm=math.sqrt(grad_norm_sq)\n",
        "\n",
        "    if grad_norm<1e-4:\n",
        "        net.zero_grad()\n",
        "        print('grad norm close to zero')\n",
        "    else:    \n",
        "        for p in net.parameters():\n",
        "            p.grad.data.div_(grad_norm)\n",
        "\n",
        "    return grad_norm\n",
        "def eval_on_test_set():\n",
        "\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "    \n",
        "    h = torch.zeros(1, bs, hidden_size)\n",
        "    \n",
        "    h=h.to(device)\n",
        "\n",
        "    \n",
        "    for count in range( 0 , 999-seq_length ,  seq_length) :\n",
        "            \n",
        "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
        "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
        "        \n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "                                \n",
        "        scores, h  = net( minibatch_data, h )\n",
        "        \n",
        "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
        "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
        "        \n",
        "        loss = criterion(  scores ,  minibatch_label )    \n",
        "        \n",
        "        h=h.detach()\n",
        "            \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1        \n",
        "    \n",
        "    total_loss = running_loss/num_batches \n",
        "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
        "    return math.exp(total_loss)\n",
        "\n",
        "start=time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
        "    if epoch >= 4:\n",
        "        my_lr = my_lr / 1.1\n",
        "    \n",
        "    # create a new optimizer and give the current learning rate.   \n",
        "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
        "        \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "    \n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(1, bs, hidden_size)\n",
        "\n",
        "    # send it to the gpu    \n",
        "    h=h.to(device)\n",
        "    \n",
        "    for count in range( 0 , 999-seq_length ,  seq_length):\n",
        "            \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
        "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
        "        \n",
        "        # send them to the gpu\n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h=h.detach()\n",
        "        # h=h.requires_grad_()\n",
        "                    \n",
        "        # forward the minibatch through the net        \n",
        "        scores, h  = net( minibatch_data, h )\n",
        "        \n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
        "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(  scores ,  minibatch_label )\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        \n",
        "            \n",
        "        # update the running loss  \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        \n",
        "        \n",
        "    # compute stats for the full training set\n",
        "    total_loss = running_loss/num_batches\n",
        "    elapsed = time.time()-start\n",
        "    \n",
        "    print('')\n",
        "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
        "    test_loss = eval_on_test_set() \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-fBx7fPezF4"
      },
      "source": [
        "## Question 3 (3 Points)\n",
        "\n",
        "Implement and train a gated recurrent unit network (GRU) on the small PTB dataset by **explicitly** implementing the GRU layer (the use of the function nn.GRU() is prohibited) :\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "r_t =& \\textrm{ sigmoid}(A x_t + a + B h_{t-1} + b)\\\\\n",
        "z_t =& \\textrm{ sigmoid}(C x_t + c + D h_{t-1} + d)\\\\\n",
        "n_t =& \\textrm{ tanh} (E x_t + e + r_t \\odot (F h_{t-1}+f))\\\\\n",
        "h_t =& (1-z_t) \\odot n_{t} + z_t \\odot h_{t-1}\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $(A,a), (B,b), (C,c), (D,d), (E,e), (F,f)$ are the parameters (matrix,bias) of all linear transformations and \n",
        "$\\odot$ is the element-wise product operator or Hadamard product. \n",
        "\n",
        "The  dataset is a subset of PTB composed of 20 sub-documents with each training document having 1000 words and each test document having 1000 words as well. \n",
        "\n",
        "__Requirements/Grading:__\n",
        "1. Explicitly implement the GRU network\n",
        "\n",
        "**Hints:** \n",
        "1. Activation function sigmoid is given by *torch.sigmoid*.\n",
        "1. The Hadamard product $\\odot$ is given by `*`.\n",
        "1. You can reuse the hyperparameters from question 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIuQzTyLezF5",
        "outputId": "7ab70701-aa9a-460e-fcda-97ab8cd8f381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestamp: 22-10-23--04-08-12\n",
            "torch.Size([1000, 20]) torch.Size([1000, 20])\n",
            "cuda\n",
            "GRU(\n",
            "  (layer1): Embedding(10000, 12)\n",
            "  (layer2): Linear(in_features=12, out_features=12, bias=True)\n",
            "  (layer3): Embedding(10000, 12)\n",
            "  (layer4): Linear(in_features=12, out_features=12, bias=True)\n",
            "  (layer5): Embedding(10000, 12)\n",
            "  (layer6): Linear(in_features=12, out_features=12, bias=True)\n",
            "  (layer7): Linear(in_features=12, out_features=10000, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "epoch= 0 \t time= 1.4147734642028809 \t lr= 1 \t exp(loss)= 1046.251382877919\n",
            "test: exp(loss) =  778.7489092133251\n",
            "\n",
            "epoch= 1 \t time= 3.30674409866333 \t lr= 1 \t exp(loss)= 642.1921689103502\n",
            "test: exp(loss) =  707.4241451600317\n",
            "\n",
            "epoch= 2 \t time= 5.193012714385986 \t lr= 1 \t exp(loss)= 545.2402684307709\n",
            "test: exp(loss) =  674.275819362502\n",
            "\n",
            "epoch= 3 \t time= 7.06289005279541 \t lr= 1 \t exp(loss)= 483.48828496338473\n",
            "test: exp(loss) =  649.2494997276248\n",
            "\n",
            "epoch= 4 \t time= 8.914753437042236 \t lr= 0.9090909090909091 \t exp(loss)= 433.0079523616955\n",
            "test: exp(loss) =  628.4037379767788\n"
          ]
        }
      ],
      "source": [
        "%reset -f\n",
        "import torch\n",
        "import datetime\n",
        "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
        "\n",
        "train_data, test_data = torch.load('dataset/small_PTB.pt')\n",
        "print(train_data.size(), test_data.size())\n",
        "\n",
        "bs = 20\n",
        "vocab_size = 10000\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "# The Now implementing GRU instead of a VRNN\n",
        "\n",
        "# YOUR CODE STARTS HERE \n",
        "class GRU(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layer1 = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.layer4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer5 = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.layer6 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer7 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, word_seq, h_init):\n",
        "        h_t = h_init\n",
        "        h_seq = []\n",
        "        for word in word_seq:\n",
        "            z = torch.sigmoid(self.layer1(word) + self.layer2(h_t))\n",
        "            r = torch.sigmoid(self.layer3(word) + self.layer4(h_t))\n",
        "            h_hat = torch.tanh(self.layer5(word) + r * self.layer6(h_t))\n",
        "            h_t = (1-z)*h_hat + z*h_t\n",
        "            h_seq.append(h_t)\n",
        "        h_seq = torch.stack(h_seq, dim=0).squeeze()\n",
        "        y_seq = self.layer7(h_seq)\n",
        "        return y_seq, h_t\n",
        "\n",
        "hidden_size = 12\n",
        "my_lr = 1\n",
        "seq_length = 3\n",
        "# YOUR CODE ENDS HERE \n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "net = GRU( hidden_size )\n",
        "\n",
        "print(net)\n",
        "\n",
        "net = net.to(device)\n",
        "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "print('')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def normalize_gradient(net):\n",
        "\n",
        "    grad_norm_sq=0\n",
        "\n",
        "    for p in net.parameters():\n",
        "        grad_norm_sq += p.grad.data.norm()**2\n",
        "\n",
        "    grad_norm=math.sqrt(grad_norm_sq)\n",
        "\n",
        "    if grad_norm<1e-4:\n",
        "        net.zero_grad()\n",
        "        print('grad norm close to zero')\n",
        "    else:    \n",
        "        for p in net.parameters():\n",
        "            p.grad.data.div_(grad_norm)\n",
        "\n",
        "    return grad_norm\n",
        "def eval_on_test_set():\n",
        "\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "    \n",
        "    h = torch.zeros(1, bs, hidden_size)\n",
        "    \n",
        "    h=h.to(device)\n",
        "\n",
        "    \n",
        "    for count in range( 0 , 999-seq_length ,  seq_length) :\n",
        "            \n",
        "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
        "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
        "        \n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "                                \n",
        "        scores, h  = net( minibatch_data, h )\n",
        "        \n",
        "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
        "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
        "        \n",
        "        loss = criterion(  scores ,  minibatch_label )    \n",
        "        \n",
        "        h=h.detach()\n",
        "            \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1        \n",
        "    \n",
        "    total_loss = running_loss/num_batches \n",
        "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
        "    return math.exp(total_loss)\n",
        "\n",
        "start=time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
        "    if epoch >= 4:\n",
        "        my_lr = my_lr / 1.1\n",
        "    \n",
        "    # create a new optimizer and give the current learning rate.   \n",
        "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
        "        \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "    \n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(1, bs, hidden_size)\n",
        "\n",
        "    # send it to the gpu    \n",
        "    h=h.to(device)\n",
        "    \n",
        "    for count in range( 0 , 999-seq_length ,  seq_length):\n",
        "            \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
        "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
        "        \n",
        "        # send them to the gpu\n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h=h.detach()\n",
        "        # h=h.requires_grad_()\n",
        "                    \n",
        "        # forward the minibatch through the net        \n",
        "        scores, h  = net( minibatch_data, h )\n",
        "        \n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
        "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(  scores ,  minibatch_label )\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        \n",
        "            \n",
        "        # update the running loss  \n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        \n",
        "        \n",
        "    # compute stats for the full training set\n",
        "    total_loss = running_loss/num_batches\n",
        "    elapsed = time.time()-start\n",
        "    \n",
        "    print('')\n",
        "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
        "    test_loss = eval_on_test_set()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (4 Points)\n",
        "\n",
        "Implement GRU-based seq2seq model with Luong attention (https://arxiv.org/pdf/1508.04025.pdf) and train the model on the french to english translation dataset. (the use of the function nn.GRU() is allowed but the attention scheme needs to be implemented **explicitly**) :\n",
        "\n",
        "The Luong attention algorithm performs the following operations:\n",
        "\n",
        "1. The encoder generates a set of hidden states, $H = \\textbf{h}_i, i = 1, 2, .....T$ , from the input sentence. The decoder generates a set of hidden states, $S = \\textbf{s}_t, t =1, 2, .....$.\n",
        "2. The current decoder hidden state is computed as: $\\textbf{s}_t = GRU_{decoder}(\\textbf{s}_{t-1}, y_{t-1})$. Here, $\\textbf{s}_{t-1}$ denotes the previous hidden decoder state, and $y_{t-1}$ the current input, which is also the expected output for the previous timestep.\n",
        "\n",
        "3. A dot product on the encoder hidden state $\\textbf{h}_i$ and the current decoder hidden state $\\textbf{s}_t$ to compute the alignment scores: $e_{t,i} = \\textbf{s}_t . \\textbf{h}_i$. \n",
        "\n",
        "4. A softmax function is applied to the alignment scores, effectively normalizing them into attention weights in a range between 0 and 1: $\\alpha_{t, i} = \\text{softmax}(e_{t, i}/ \\textbf{e}_t)$.\n",
        "\n",
        "5. These attention weights together with the encoder hidden states are used to generate a context vector through a weighted sum: $\\textbf{c}_t = \\sum_{i=1}^T\\alpha_{t, i}\\textbf{h}_i$.\n",
        "\n",
        "6. An attentional hidden state is computed based on a weighted concatenation of the context vector and the current decoder hidden state: $\\tilde{\\textbf{s}_t} = \\text{tanh}\\big(W_c\\big[\\textbf{c}_t; \\textbf{s}_t\\big]\\big)$.\n",
        "\n",
        "7. The decoder produces a final output by feeding it a weighted attentional hidden state: $y_t = \\text{softmax}(W_y\\tilde{\\textbf{s}_t})$.\n",
        "\n",
        "8. Steps 2-7 are repeated until the end of the sequence. \n",
        "\n",
        "The attention has to be calculated in parallel via matrix multiplication. For loop $\\textbf{should not}$ be used.\n",
        "\n",
        "__Requirements/Grading:__\n",
        "1. Implement the forward pass for the network in the train and eval function.\n",
        "1. Explicitly implement the attention network (Use of nn.GRU() is allowed).\n",
        "\n",
        "**Hints:** \n",
        "1. torch.swapaxes to convert from [seq_len, bs, hidden_size] to [bs, seq_len, hidden_size].\n",
        "1. torch.bmm to perform batch matrix multiplication\n",
        "1. torch.concat to concatenate $c_t$ and $s_t$\n",
        "1. Training took me around ~1 minute per epoch\n"
      ],
      "metadata": {
        "id": "2F_kTNpUg_tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Iterable, List\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# For this dataset, we are trying to translate french to english\n",
        "SRC_LANGUAGE = 'fr'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# First, we create a custom dataset to load the data. Each item is a pair of french and english datapoint\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, train, train_size=10000, test_size=1000, max_len=50):\n",
        "        self.en_dir = os.path.join(\"dataset\", \"europarl-v7.fr-en.en\")\n",
        "        self.fr_dir = os.path.join(\"dataset\", \"europarl-v7.fr-en.fr\")\n",
        "        with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
        "            self.english_data = f.readlines()\n",
        "        with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
        "            self.french_data = f.readlines()\n",
        "        # Only train on sentences with less than 50 letters\n",
        "        self.indicies = np.array([i for i in range(len(self.english_data)) if len(self.english_data[i]) < max_len])\n",
        "        # First 10000 datapoints for train\n",
        "        if train:\n",
        "          with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
        "              self.english_data = [self.english_data[i] for i in self.indicies][:train_size]\n",
        "          with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
        "              self.french_data = [self.french_data[i] for i in self.indicies][:train_size]\n",
        "        # Next 10000 datatpoints for test\n",
        "        else:\n",
        "            with open(self.en_dir, \"r\", encoding=\"utf8\") as f:\n",
        "                self.english_data = [self.english_data[i] for i in self.indicies][train_size:train_size+test_size]\n",
        "            with open(self.fr_dir, \"r\", encoding=\"utf8\") as f:\n",
        "                self.french_data = [self.french_data[i] for i in self.indicies][train_size:train_size+test_size]\n",
        "                # self.french_data = f.readlines()[train_size:train_size+test_size]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.french_data[idx], self.english_data[idx]\n",
        "\n",
        "# Instantiate dataset\n",
        "dataset = CustomDataset(train=True)\n",
        "\n",
        "# Next, we load the tokenizer that transforms the input sentence into tokens\n",
        "token_transform = {}\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='fr_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# Helper function to call token_transform\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "\n",
        "# Next, we build the dictionary to convert the tokens to indicies.\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocab_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = iter(dataset)\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "\n",
        "\n",
        "# Functions transform the input sentence to a format that can be used for training \n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "def collate_fn(src, tgt):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in zip(src, tgt):\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch\n"
      ],
      "metadata": {
        "id": "_mz8m4f5iwWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0428e53d-e7c3-46cf-a9b8-5c6add28e4d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 429 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print an example\n",
        "batch_size = 8\n",
        "dataset = CustomDataset(train=True)\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "fr_sentence, eng_sentence = next(iter(train_dataloader)) \n",
        "print(f\"Raw Inputs: {fr_sentence[0]}\\n{eng_sentence[0]}\")\n",
        "# First we split the sentence into tokens\n",
        "fr_token, eng_token = [token_transform[\"fr\"](i.rstrip(\"\\n\")) for i in fr_sentence], [token_transform[\"en\"](i.rstrip(\"\\n\")) for i in eng_sentence]\n",
        "print(f\"Tokenized Inputs: {fr_token[0]}\\n{eng_token[0]}\")\n",
        "# # Next we transform the tokens into numbers\n",
        "fr_idx, eng_idx = [vocab_transform[\"fr\"](i) for i in fr_token], [vocab_transform[\"en\"](i) for i in eng_token]\n",
        "print(f\"Tokenized Inputs to indicies: {fr_idx[0]}\\n{eng_idx[0]}\")\n",
        "# # Next, we add the beginning of sentence, end of sentence\n",
        "fr_pad, eng_pad = [tensor_transform(i) for i in fr_idx], [tensor_transform(i) for i in eng_idx]\n",
        "print(f\"Tokenized Indicies with begin (2) and end token (3): {fr_pad[0]}\\n{eng_pad[0]}\")\n",
        "# # Lastly, we pad the rest of the sentence\n",
        "# This also changes the shape from (bs, seq_len) to (seq_len, bs)\n",
        "fr_pad, eng_pad = pad_sequence(fr_pad, padding_value=PAD_IDX), pad_sequence(eng_pad, padding_value=PAD_IDX)\n",
        "print(f\"After padding (1): {fr_pad[:, 0]}\\n{eng_pad[:, 0]}\")\n",
        "\n",
        "# All the above is combined into collate_fn\n",
        "x, y = collate_fn(fr_sentence, eng_sentence)\n",
        "print(f\"Same Outputs: {x[:, 0]}\\n{y[:, 0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99DcG-uAGa3b",
        "outputId": "806f4b34-6297-413b-94e8-21ed0885ff9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Inputs: La troisième incohérence est liée aux droits de l'homme.\n",
            "\n",
            "The third area of incoherence is human rights.\n",
            "\n",
            "Tokenized Inputs: ['La', 'troisième', 'incohérence', 'est', 'liée', 'aux', 'droits', 'de', \"l'\", 'homme', '.']\n",
            "['The', 'third', 'area', 'of', 'incoherence', 'is', 'human', 'rights', '.']\n",
            "Tokenized Inputs to indicies: [34, 1226, 3007, 6, 3065, 156, 311, 7, 18, 251, 5]\n",
            "[11, 726, 409, 12, 5180, 6, 310, 246, 5]\n",
            "Tokenized Indicies with begin (2) and end token (3): tensor([   2,   34, 1226, 3007,    6, 3065,  156,  311,    7,   18,  251,    5,\n",
            "           3])\n",
            "tensor([   2,   11,  726,  409,   12, 5180,    6,  310,  246,    5,    3])\n",
            "After padding (1): tensor([   2,   34, 1226, 3007,    6, 3065,  156,  311,    7,   18,  251,    5,\n",
            "           3])\n",
            "tensor([   2,   11,  726,  409,   12, 5180,    6,  310,  246,    5,    3,    1,\n",
            "           1,    1])\n",
            "Same Outputs: tensor([   2,   34, 1226, 3007,    6, 3065,  156,  311,    7,   18,  251,    5,\n",
            "           3])\n",
            "tensor([   2,   11,  726,  409,   12, 5180,    6,  310,  246,    5,    3,    1,\n",
            "           1,    1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "# Hyperparameters\n",
        "num_epochs = 5\n",
        "hidden_size = 256\n",
        "my_lr = 1.3\n",
        "bs = 32\n",
        "\n",
        "# Variables\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "train_dataset = CustomDataset(train=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "test_dataset = CustomDataset(train=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "\n",
        "def eval_on_test_set():\n",
        "\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "    \n",
        "    h = torch.zeros(1, bs, hidden_size)\n",
        "    \n",
        "    h=h.to(device)\n",
        "\n",
        "    for x, y in test_dataloader:\n",
        "        x, y = collate_fn(x, y)\n",
        "        # Batch size might be different for the last batch\n",
        "        batch_size = x.size()[1]\n",
        "        seq_length = y.size()[0] - 1\n",
        "        # set the initial h to be the zero vector\n",
        "        h = torch.zeros(1, batch_size, hidden_size)\n",
        "        # send them to the gpu\n",
        "        minibatch_data=x.type(torch.LongTensor).to(device)\n",
        "        minibatch_label=y.type(torch.LongTensor).to(device)\n",
        "        h=h.to(device)\n",
        "\n",
        "        # FILL UP FORWARD PASS\n",
        "        minibatch_label_input = minibatch_label[:-1]\n",
        "        scores = net.forward(minibatch_data, minibatch_label_input, h)\n",
        "        loss = criterion(scores.reshape(-1, scores.shape[2]), minibatch_label[1:].reshape(-1))\n",
        "        # END OF FORWARD PASS\n",
        "\n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # update the running loss  \n",
        "        running_loss += loss.detach().item()\n",
        "        num_batches += 1\n",
        "        # Collect garbage to prevent OOM\n",
        "        gc.collect()\n",
        "    \n",
        "    total_loss = running_loss/num_batches \n",
        "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
        "    return math.exp(total_loss)\n",
        "\n",
        "# Fill UP ATTENTION NETWORK\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.xembedding = nn.Embedding(SRC_VOCAB_SIZE, hidden_size)\n",
        "        self.yembedding = nn.Embedding(SRC_VOCAB_SIZE, hidden_size)\n",
        "        self.encoder = nn.GRU(hidden_size, hidden_size)\n",
        "        self.decoder = nn.GRU(hidden_size, hidden_size)\n",
        "        self.Wc = nn.Linear(2*hidden_size, hidden_size)\n",
        "        self.Wy = nn.Linear(hidden_size, TGT_VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, x, y, h_init):\n",
        "        x = self.xembedding(x)\n",
        "        y = self.yembedding(y)\n",
        "        h_seq, h = self.encoder(x, h_init)\n",
        "        s_seq, s = self.decoder(y, h)\n",
        "        alignment_scores = torch.bmm(s_seq.swapaxes(0, 1), h_seq.swapaxes(0, 1).swapaxes(1, 2))\n",
        "        attention_weights = F.softmax(alignment_scores, dim=2)\n",
        "        context_vector = torch.bmm(attention_weights, h_seq.swapaxes(0,1))\n",
        "        attentional_hidden_state = torch.tanh(self.Wc(torch.cat((context_vector, s_seq.swapaxes(0,1)), dim=2)))\n",
        "        scores = self.Wy(attentional_hidden_state)\n",
        "        y_out = F.log_softmax(scores, dim=2).swapaxes(0,1)\n",
        "        return y_out\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "net = LuongAttention( hidden_size )\n",
        "\n",
        "print(net)\n",
        "\n",
        "net = net.to(device)\n",
        "\n",
        "print('')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def normalize_gradient(net):\n",
        "\n",
        "    grad_norm_sq=0\n",
        "\n",
        "    for p in net.parameters():\n",
        "        grad_norm_sq += p.grad.data.norm()**2\n",
        "\n",
        "    grad_norm=math.sqrt(grad_norm_sq)\n",
        "\n",
        "    if grad_norm<1e-4:\n",
        "        net.zero_grad()\n",
        "        print('grad norm close to zero')\n",
        "    else:    \n",
        "        for p in net.parameters():\n",
        "            p.grad.data.div_(grad_norm)\n",
        "\n",
        "    return grad_norm\n",
        "\n",
        "start=time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "      # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
        "    if epoch >= 4:\n",
        "        my_lr = my_lr / 1.1\n",
        "    \n",
        "    # create a new optimizer and give the current learning rate.   \n",
        "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
        "        \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss=0\n",
        "    num_batches=0    \n",
        "\n",
        "    for x, y in train_dataloader:\n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        # Transform inputs\n",
        "        x, y = collate_fn(x, y)\n",
        "        # Batch size might be different for the last batch\n",
        "        batch_size = x.size()[1]\n",
        "        seq_length = y.size()[0] - 1\n",
        "        # set the initial h to be the zero vector\n",
        "        h = torch.zeros(1, batch_size, hidden_size)\n",
        "        # send them to the gpu\n",
        "        minibatch_data=x.type(torch.LongTensor).to(device)\n",
        "        minibatch_label=y.type(torch.LongTensor).to(device)\n",
        "        h=h.to(device)\n",
        "\n",
        "        # FILL UP FORWARD PASS\n",
        "        minibatch_label_input = minibatch_label[:-1]\n",
        "        scores = net.forward(minibatch_data, minibatch_label_input, h)\n",
        "        loss = criterion(scores.reshape(-1, scores.shape[2]), minibatch_label[1:].reshape(-1))\n",
        "        # END OF FORWARD PASS\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        # update the running loss  \n",
        "        running_loss += loss.detach().item()\n",
        "        num_batches += 1\n",
        "        # Collect garbage to prevent OOM\n",
        "        gc.collect()\n",
        "    # compute stats for the full training set\n",
        "    total_loss = running_loss / num_batches\n",
        "    elapsed = time.time() - start\n",
        "    \n",
        "    print('')\n",
        "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
        "    eval_on_test_set()"
      ],
      "metadata": {
        "id": "61CLe7y-x9P7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c95b82-daac-4f40-cf6e-d2faa5f13c0c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "LuongAttention(\n",
            "  (xembedding): Embedding(7797, 256)\n",
            "  (yembedding): Embedding(7797, 256)\n",
            "  (encoder): GRU(256, 256)\n",
            "  (decoder): GRU(256, 256)\n",
            "  (Wc): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (Wy): Linear(in_features=256, out_features=6278, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "epoch= 0 \t time= 67.82932162284851 \t lr= 1.3 \t exp(loss)= 23.054626146711854\n",
            "test: exp(loss) =  13.34057459509136\n",
            "\n",
            "epoch= 1 \t time= 142.05984783172607 \t lr= 1.3 \t exp(loss)= 9.574782299727866\n",
            "test: exp(loss) =  10.216556111528563\n",
            "\n",
            "epoch= 2 \t time= 216.2738425731659 \t lr= 1.3 \t exp(loss)= 7.011261567723664\n",
            "test: exp(loss) =  9.034546320929667\n",
            "\n",
            "epoch= 3 \t time= 291.57278537750244 \t lr= 1.3 \t exp(loss)= 5.502538829732504\n",
            "test: exp(loss) =  8.724199257905898\n",
            "\n",
            "epoch= 4 \t time= 366.6670711040497 \t lr= 1.1818181818181817 \t exp(loss)= 4.443762199552307\n",
            "test: exp(loss) =  8.736323903039871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "count = 0\n",
        "for x, y in test_dataloader:\n",
        "    print(x)\n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(1, 1, hidden_size)\n",
        "\n",
        "    # send it to the gpu    \n",
        "    h=h.to(device)\n",
        "    x, y = collate_fn(x, y)\n",
        "    # send them to the gpu\n",
        "    minibatch_data=x.type(torch.LongTensor).to(device)\n",
        "    # The first prediction is the start of sentence index\n",
        "    start_index = torch.tensor([[2]]).type(torch.LongTensor).to(device)\n",
        "    predictions=start_index\n",
        "    for _ in range(20):\n",
        "      # At every loop, pass in the previous predictions\n",
        "      predictions = net.forward(minibatch_data, predictions, h)\n",
        "      predictions = torch.reshape(predictions, (-1, TGT_VOCAB_SIZE, 1))\n",
        "      # Get the new predictions shifted right by 1 timestep\n",
        "      predictions = torch.argmax(predictions, dim=1)\n",
        "      # Add back the first timestep\n",
        "      predictions = torch.cat([start_index, predictions], 0)\n",
        "      if predictions[-1].item() == 3:\n",
        "          break\n",
        "    predictions = predictions.reshape(-1)\n",
        "    # Transform from token to words\n",
        "    predictions = [vocab_transform[TGT_LANGUAGE].lookup_token(i) for i in predictions]\n",
        "    print(f\"Label: {[vocab_transform[TGT_LANGUAGE].lookup_token(i) for i in y]}\")\n",
        "    print(f\"Predicted: {predictions}\")\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "RijG2UdmSTdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1260a539-5319-434e-edad-b7e18c5d76bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Mais qui a peur ?\\n',)\n",
            "Label: ['<bos>', 'But', 'who', 'fears', ',', 'exactly', '?', '<eos>']\n",
            "Predicted: ['<bos>', 'But', 'what', 'is', 'this', '?', '<eos>']\n",
            "('Sapristi, mais quelle Europe est-ce donc ? !\\n',)\n",
            "Label: ['<bos>', 'For', '<unk>', \"'s\", 'sake', '!', 'What', 'sort', 'of', 'Europe', 'is', 'this', '?', '<eos>']\n",
            "Predicted: ['<bos>', 'So', 'this', 'is', 'not', 'what', 'is', 'done', ',', 'but', 'it', 'did', 'that', '!', '<eos>']\n",
            "('Pourquoi cette omission ?\\n',)\n",
            "Label: ['<bos>', 'Why', 'is', 'this', '?', '<eos>']\n",
            "Predicted: ['<bos>', 'What', 'does', 'this', 'mean', '?', '<eos>']\n",
            "('Ce sera ma première question.\\n',)\n",
            "Label: ['<bos>', 'That', 'is', 'my', 'first', 'question', '.', '<eos>']\n",
            "Predicted: ['<bos>', 'That', 'is', 'my', 'first', 'question', '.', '<eos>']\n",
            "(\"Nous sommes d'accord !\\n\",)\n",
            "Label: ['<bos>', 'We', 'agree', '.', '<eos>']\n",
            "Predicted: ['<bos>', 'We', 'are', 'not', 'agree', 'with', 'that', '.', '<eos>']\n",
            "(\"Je suis encore bouleversé et choqué par ce que j'ai vu.\\n\",)\n",
            "Label: ['<bos>', 'I', 'am', 'still', '<unk>', 'from', 'the', 'shock', '.', '<eos>']\n",
            "Predicted: ['<bos>', 'I', 'am', 'sure', 'I', 'am', 'sure', 'I', 'am', 'sure', 'I', 'am', 'sure', 'I', 'am', 'sure', 'I', 'am', 'sure', 'I', 'am']\n",
            "(\"Heureusement, personne n'a été blessé.\\n\",)\n",
            "Label: ['<bos>', 'Fortunately', 'no', 'one', 'was', '<unk>', '.', '<eos>']\n",
            "Predicted: ['<bos>', 'In', 'this', 'case', ',', 'however', ',', 'has', 'been', 'been', 'done', '.', '<eos>']\n",
            "('Peut-être comprendront-ils mieux.\\n',)\n",
            "Label: ['<bos>', 'Perhaps', 'they', 'would', 'come', 'to', 'understand', 'better', '.', '<eos>']\n",
            "Predicted: ['<bos>', 'They', 'want', 'to', 'be', 'brief', '.', '<eos>']\n",
            "(\"Les Européens n'en ont aucune idée.\\n\",)\n",
            "Label: ['<bos>', 'People', 'of', 'Europe', 'have', 'no', 'idea', 'of', 'this', '.', '<eos>']\n",
            "Predicted: ['<bos>', 'The', 'other', 'people', 'are', 'not', 'just', '.', '<eos>']\n",
            "('Je donne entièrement raison au Commissaire Patten.\\n',)\n",
            "Label: ['<bos>', 'I', 'completely', 'agree', 'with', 'Commissioner', 'Patten', '.', '<eos>']\n",
            "Predicted: ['<bos>', 'I', 'am', 'very', 'grateful', 'for', 'your', 'Commissioner', '.', '<eos>']\n",
            "('Il ne peut en être ainsi !\\n',)\n",
            "Label: ['<bos>', 'This', 'is', 'completely', 'unacceptable', '.', '<eos>']\n",
            "Predicted: ['<bos>', 'That', 'can', 'not', 'be', 'in', 'any', 'way', '!', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MWjgSSE1wBOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}